{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf \n",
    "import matplotlib.gridspec as gridspec\n",
    "import math\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data[\"Class\"].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Column for non-fraud transaction\n",
    "data.loc[data.Class == 0, 'normal'] = 1\n",
    "data.loc[data.Class == 1, 'normal'] = 0\n",
    "\n",
    "# Rename the Class to fraud\n",
    "\n",
    "data = data.rename(columns ={'Class' : 'fraud'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Time', 'Amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = data[data.fraud == 1]\n",
    "normal = data[data.normal ==1]\n",
    "fraud.shape\n",
    "normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "normal =resample(normal, replace=False, #sample without replacement\n",
    "                         n_samples =492, # to match with minority class\n",
    "                         random_state =123 #reproducible results\n",
    "                )\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X_train to 80% of fraduelent transactions\n",
    "X_train = fraud.sample(frac =0.8)\n",
    "count_frauds =len(X_train)\n",
    "# Add 80% of normal tansactions to X_train\n",
    "X_train = pd.concat([X_train, normal.sample(frac=0.8)], axis =0)\n",
    "# X_test now contains all the transactions not in X_train\n",
    "X_test = data.loc[~data.index.isin(X_train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# shuffle the dataframe so that training can be done in a random fashion\n",
    "X_train = shuffle(X_train)\n",
    "X_test = shuffle(X_test)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding our target features to y_train and y_test\n",
    "y_train = X_train.fraud\n",
    "y_train = pd.concat([y_train, X_train.normal], axis =1)\n",
    "\n",
    "y_test = X_test.fraud\n",
    "y_test = pd.concat([y_test, X_test.normal], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping target features from X_train & X_test\n",
    "X_train = X_train.drop(['fraud', 'normal'], axis =1)\n",
    "X_test = X_test.drop(['fraud', 'normal'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "# getting required i/p & o/p matrices\n",
    "x_input_train = X_train.as_matrix()\n",
    "y_input_train = y_train.as_matrix()\n",
    "\n",
    "x_input_test = X_test.as_matrix()\n",
    "y_input_test = y_test.as_matrix()\n",
    "\n",
    "X_test.shape\n",
    "y_train.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Defining model hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size =100\n",
    "\n",
    "#placeholders\n",
    "x =tf.placeholder(tf.float32,[None,29], name= 'x_placeholder')\n",
    "y =tf.placeholder(tf.float32,[None,2], name= 'y_placeholder')\n",
    "\n",
    "num_classes = 2 \n",
    "feature_num = 29\n",
    "#hidden laye sizes\n",
    "hidden1_unit = 29\n",
    "hidden2_unit = 10\n",
    "hidden3_unit = 10\n",
    "hidden4_unit = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "# Model layers\n",
    "weights1= tf.Variable(tf.truncated_normal([29, hidden1_unit], stddev=1.0/math.sqrt(float(feature_num))), name = 'weights')\n",
    "biases1 = tf.Variable(tf.zeros(hidden1_unit), name='biases')\n",
    "hidden1 = tf.nn.sigmoid(tf.matmul(x,weights1)*biases1)\n",
    "hidden1 = tf.nn.dropout(hidden1, 0.5)\n",
    "\n",
    "weights2= tf.Variable(tf.truncated_normal([hidden1_unit, hidden2_unit], stddev=1.0/math.sqrt(float(hidden1_unit))), name = 'weights')\n",
    "biases2 = tf.Variable(tf.zeros(hidden2_unit), name='biases')\n",
    "hidden2 = tf.nn.sigmoid(tf.matmul((hidden1_unit),(weights2))*biases2)\n",
    "hidden2 = tf.nn.dropout(hidden2, 0.5)\n",
    "\n",
    "weights3= tf.Variable(tf.truncated_normal([hidden2_unit, hidden3_unit], stddev=1.0/math.sqrt(float(hidden2_unit))), name = 'weights')\n",
    "biases3 = tf.Variable(tf.zeros(hidden3_unit), name='biases')\n",
    "hidden3 = tf.nn.sigmoid(tf.matmul((hidden2_unit),(weights3))*biases3)\n",
    "hidden3 = tf.nn.dropout(hidden3, 0.5)\n",
    "\n",
    "#Linear\n",
    "\n",
    "weights4= tf.Variable(tf.truncated_normal([hidden3_unit, num_classes], stddev=1.0/math.sqrt(float(hidden5_unit))), name = 'weights')\n",
    "biases4 = tf.Variable(tf.zeros(num_classes), name='biases')\n",
    "logits = tf.nn.softmax(tf.matmul(hidden3, weights5) +biases5)\n",
    "\n",
    "loss =tf.cast((tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits =logits, labels=Y)) +\n",
    "                                                                      0.01*tf.nn.l2_loss(weights1)+\n",
    "                                                                      0.01*tf.nn.l2_loss(biases)+\n",
    "                                                                      0.01*tf.nn.l2_loss(weights4)+\n",
    "                                                                      0.01*tf.nn.l2_loss(biases)), tf.float32)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "train_predictions = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    # return a total no of 'num' random samples with labels\n",
    "    idx = np.arange(0, len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx =idx[:num]\n",
    "    data_shuffle =[data[i] for i in idx]\n",
    "    labels_shuffle =[labels[i] for i in idx]\n",
    "    \n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session\n",
    "import datetime\n",
    "sess = tf.InteractiveSession()\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "time_string = datetime.datetime.now().isoformat()\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(f'./train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter(f'./test', sess.graph)\n",
    "\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Session \n",
    "\n",
    "print(\"Initialized\")\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "d=[]\n",
    "\n",
    "for step in range(2000): #epochs\n",
    "    #defining offset\n",
    "    offset = (step*batch_size)%(y_input_train.shape[0] - batch_size)\n",
    "    #creating batch data and labels\n",
    "    batch_data =x_input_train[offset:(offset + batch_size)]\n",
    "    batch_labels = y_input_train[offset:(offset + batch_size)]\n",
    "    \n",
    "    batch_x, batch_y = batch_data, batch_labels\n",
    "    sess.run(optimizer, feed_dict= {X: batch_x, Y: batch_y})\n",
    "    sess.run(accuracy, feed_dict= {X: x_input_test, Y: y_input_test}\n",
    "    \n",
    "    if step%50 ==0:\n",
    "             train =sess.run(accuracy, feed_dict ={X: batch_x, Y: batch_y})\n",
    "             test = sess.run(accuracy, feed_dict ={X: x_input_test, Y: y_input_test})\n",
    "             l_train = sess.run(loss, feed_dict = {X: batch_x, Y: batch_y})\n",
    "             l_test = sess.run(loss, feed_dict ={X: x_input_test, Y: y_input_test}\n",
    "                               \n",
    "                               \n",
    "    a.append(train)\n",
    "    b.append(test)\n",
    "    c.append(l_train)\n",
    "    d.append(l_test)\n",
    "    \n",
    "    print(f'step: {step}')\n",
    "    print('model training accuracy:')\n",
    "    print(sess.run(accuracy, feed_dict ={X: batch_x, Y: batch_y}))                           \n",
    "    print('model test accuracy:')      \n",
    "    print(sess.run(accuracy, feed_dict ={X: x_input_test, Y: y_input_test}))                           \n",
    "    \n",
    "print('final model accuracy')\n",
    "print(sess.run(accuracy, feed_dict ={X: x_input_test, Y: y_input_test}))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a, label='Train Accuracy')\n",
    "plt.plot(b.linestyle='--', color='r', label='Test Accuracy')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(c, label='Training Loss')\n",
    "plt.plot(d.linestyle='--', color='r', label='Test Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
